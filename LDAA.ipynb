{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9540a5",
   "metadata": {},
   "source": [
    "## LDA 토픽 모델링으로 콘텐츠 리뷰를 분석하자\n",
    "\n",
    "https://velog.io/@mare-solis/LDA-%ED%86%A0%ED%94%BD-%EB%AA%A8%EB%8D%B8%EB%A7%81%EC%9C%BC%EB%A1%9C-%EC%BD%98%ED%85%90%EC%B8%A0-%EB%A6%AC%EB%B7%B0%EB%A5%BC-%EB%B6%84%EC%84%9D%ED%95%98%EC%9E%90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d3a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from tqdm import tqdm \n",
    "import re\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "\n",
    "#from konlpy.tag import Okt\n",
    "#okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ccad22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamiejeong-eunpark/opt/anaconda3/envs/mecab/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58906761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('comment_product_mapped.csv', low_memory=False)\n",
    "data = pd.read_csv('data/과일견과쌀.csv', low_memory=False)\n",
    "#정육계란, 채소, 수산해산, 과일견과쌀 - 81만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111b52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공백 삭제\n",
    "data['리뷰 내용'] = data['리뷰 내용'].str.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "# 정규표현식으로 한글이 아닌 문자 찾기\n",
    "non_korean_pattern = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣]+')\n",
    "data['리뷰 내용'] = data['리뷰 내용'].astype(str)\n",
    "data['exclamation_mark'] = data['리뷰 내용'].apply(lambda x: non_korean_pattern.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e0fd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [~, ~~]\n",
       "1                           [!]\n",
       "2                            []\n",
       "3                            []\n",
       "4                       [., ^^]\n",
       "                  ...          \n",
       "908742                     [!!]\n",
       "908743                      [.]\n",
       "908744                   [., .]\n",
       "908745    [!, ., ., ,, ,, ., ~]\n",
       "908746                       []\n",
       "Name: exclamation_mark, Length: 908747, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['exclamation_mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902c3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\".\", \" \").strip()\n",
    "    text = text.replace(\"·\", \" \").strip()\n",
    "    pattern = '[^ ㄱ-ㅣ가-힣|0-9]+'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text) #한글, 숫자만 남김\n",
    "    return text\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.replace(\".\", \" \").strip()\n",
    "#     text = text.replace(\"·\", \" \").strip()\n",
    "#     pattern = '[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]'\n",
    "#     text = re.sub(pattern=pattern, repl=' ', string=text) #한글, 숫자만 남김\n",
    "#     words = text.split()\n",
    "#     #print(words)\n",
    "#     words = [word for word in words if not any(keyword in word for keyword in ['감자', '컬리', '마켓'])]\n",
    "#     print(words)\n",
    "#     #words = [word for word in words if ['감자','컬리', '마켓'] not in word]\n",
    "#     clean_text = ' '.join(words)\n",
    "#     return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c05592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(tokenizer, sentence):\n",
    "    tagged = tokenizer.pos(sentence)\n",
    "    #meaningful = [s for s, t in tagged if t in ['Noun', 'Verb', 'PreEomi', 'Unknown']]\n",
    "    nouns = [s for s, t in tagged if t in ['NNG', 'NNP', 'VA', 'XR'] and len(s) >1]\n",
    "    return nouns\n",
    "\n",
    "def tokenize(df):\n",
    "    tokenizer = Mecab() #오타 수정 기능도 있는 okt 선택. \n",
    "    processed_data = []\n",
    "    for sent in tqdm(df['review_contents']):\n",
    "        \n",
    "        sentence = clean_text(str(sent).replace(\"\\n\", \"\").strip())\n",
    "        #processed_data.append(get_meaningful(tokenizer, sentence))\n",
    "        processed_data.append(get_nouns(tokenizer, sent))\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0f002",
   "metadata": {},
   "source": [
    "### [Python] 한국어 형태소 분석기 체험 및 비교(Okt, Mecab, Komoran, Kkma)\n",
    "https://soohee410.github.io/compare_tagger\n",
    "\n",
    "'재밓었다' 걍 다 뭉개짐. 띄어쓰기도 안 되고, 크리티컬한 오타가 존재할 때, 형태소 분석이 잘못될 가능성이 매우 높아지는 것 같다. Kkma와 Komoran은 더 구체적으로 형태소에 따라 단어를 쪼갬. \n",
    "\n",
    "Okt의 경우 stem=True, norm=True의 파라미터가 존재해서, 단어들을 알아서 조금 정규화해주고, 오타도 조금 수정해주는 기능이 있다. ‘사랑하고’ -> ‘사랑하다’로, ‘싶게’->싶다’로, ‘헤집어놓는’->‘헤집다’\n",
    "\n",
    "kkma 너무 오래 걸림."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d140c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'걍 다 뭉개짐  띄어쓰기도 안 되고 크리티컬한 오타가 존재할 때 형태소 분석이 잘못될 가능성이 매우 높아지는 것 같다  와 은 더 구체적으로 형태소에 따라 단어를 쪼갬'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(' 걍 다 뭉개짐. 띄어쓰기도 안 되고, 크리티컬한 오타가 존재할 때, 형태소 분석이 잘못될 가능성이 매우 높아지는 것 같다. Kkma와 Komoran은 더 구체적으로 형태소에 따라 단어를 쪼갬.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8adaf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▏                              | 85791/908747 [00:21<01:42, 8063.68it/s]"
     ]
    }
   ],
   "source": [
    "def save_processed_data(processed_data):\n",
    "    with open(\"_Mecabtokenized_data_\"+file, 'w', newline=\"\", encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for data in processed_data:\n",
    "            writer.writerow(data)\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #df = pd.read_csv(file, low_memory=False)\n",
    "    df = data\n",
    "    df.columns=['product_code', 'buyer_class', 'review_best_status', 'review_contents', 'review_letNum', 'pic_status', 'pics_num', 'helpful_num', 'review_reg_date', 'review_exp_date', 'product_name', 'product_name_letNum', 'brand', 'price', 'discount_price', 'discount_rate', 'product_phrase', 'coupon_status','coupon_discount_rate', 'product_characteristic', 'delivery_type', 'category', 'purchase_benefit', 'total_review_num', 'overwhelming_review_num', 'exclamation_mark']\n",
    "    df.dropna(how='any')\n",
    "\n",
    "    processed_data = tokenize(df)\n",
    "    save_processed_data(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb26aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = 'comment_product_mapped.csv'\n",
    "file = '과일견과쌀.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [sent.strip().split(\",\") for sent in tqdm(open(\"_Mecabtokenized_data_\"+file,'r',encoding='utf-8').readlines())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = DataFrame(processed_data)\n",
    "processed_data[0] = processed_data[0].replace(\"\", np.nan)\n",
    "processed_data = processed_data[processed_data[0].notnull()]\n",
    "processed_data = processed_data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4361ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_data2=[]\n",
    "for i in tqdm(processed_data):\n",
    "    i = list(filter(None, i))\n",
    "    processed_data2.append(i)\n",
    "processed_data = processed_data2\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9892305",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ['감자', '컬리', '마켓', '아요', '요조', '구매', '오이', '채소', '야채', '새우', '오징어', '딸기', '블루베리', '바나나', '아보카도', '귤', '머스켓','머스캣'] #구매?\n",
    "processed2 = []\n",
    "for sent in processed_data[:10]:\n",
    "    temp_sent = []\n",
    "    for word in sent:\n",
    "        if word not in exclude:\n",
    "            temp_sent.append(word)\n",
    "    processed2.append(temp_sent)\n",
    "\n",
    "processed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddfd20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.callbacks import CoherenceMetric\n",
    "from gensim import corpora\n",
    "from gensim.models.callbacks import PerplexityMetric\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dcf7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b55901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "\n",
    "#너무 흔한 단어나 너무 독특한 단어는 제외\n",
    "#빈도가 1인 단어가 포함되면 너무 많은 단어가 생겨 분석에 혼선을 줄 수 있음\n",
    "#따라서 저는 빈도가 2 이상인 단어와 전체의 50%로 이상 차지하는 단어는 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in tqdm(processed2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc29b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df116b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "temp = dictionary[0]\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5698db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25deb12",
   "metadata": {},
   "source": [
    "bert topic & combined tm  중 더 잘 되는 거 고르기\n",
    "\n",
    "평가기준. f1 x. 어떤 기준으로 뭐가 더 잘되는지 골라야하는가. \n",
    "도메인에 따라 어떤 가중치를 주는가.\n",
    "다음에 찾아올 때는 피드백들 발전시킬 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_visualization = gensimvis.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.save_html(lda_visualization, 'fruits_revised_topic3.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519c2b8",
   "metadata": {},
   "source": [
    "## 토픽별 element 출력해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51761eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_word(text):\n",
    "    replaced_text = text.replace('요', '요 ')\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bfbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_word('최저가라서샀어요조아요')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfc7343",
   "metadata": {},
   "source": [
    "채소 - 4,5 topic modeling -> 완전 동일한(겹치는) 뭉치들의 토픽들이 있다. 이걸 제외하고 생각한다면 잘 나눠진 것 같다. chaeso_revised_topics00.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0f148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
